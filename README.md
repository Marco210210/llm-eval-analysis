# LLM-Eval â€“ Automatic Evaluation of Dialogues with LLMs

**LLM-Eval** is a two-phase research project that explores how Large Language Models (LLMs) can be used to **automatically evaluate the quality of dialogues** between humans and conversational agents.

The project investigates the use of the **LLM-EVAL framework**, testing its ability to reproduce human-like evaluations across different models and datasets.

---

## ğŸŒ Project Overview

- Phase 1: Evaluate four LLMs on a benchmark dataset (`ConvAI2`):
  - Claude 3
  - Claude 3.5
  - GPT-4o
  - GPT-4o-mini
- Phase 2: Evaluate how dataset structure affects performance (using Claude 3):
  - FED
  - PC
  - TC
  - DSTC9
- Metrics: Accuracy, Cohenâ€™s Kappa, Pearson, Spearman, Kendall-Tau correlations
- Evaluation schema follows the paper: *LLM-Eval: Unified Multi-Dimensional Automatic Evaluation for Open-Domain Conversations*

---

## ğŸ› ï¸ Technologies & Tools

- **Programming Language**: Python 3
- **API Access**: OpenAI + Anthropic APIs
- **Environment Management**: `venv` + `.env` for keys
- **Libraries**: `json`, `os`, `tqdm`, `anthropic`, `openai`, `sklearn`, `pandas`, `matplotlib`

---

## ğŸ“ Repository Structure

```plaintext
LLM-Eval/
â”œâ”€â”€ docs/                    â†’ Project report, presentation, paper
â”œâ”€â”€ prog/
â”‚   â”œâ”€â”€ dataset1/            â†’ Phase 1: Model-based evaluation (Claude, GPT)
â”‚   â”‚   â”œâ”€â”€ Claude3/
â”‚   â”‚   â”œâ”€â”€ Claude3-5/
â”‚   â”‚   â”œâ”€â”€ GPT-4o/
â”‚   â”‚   â”œâ”€â”€ GPT-4o-mini/
â”‚   â”‚   â””â”€â”€ convai2_data.json
â”‚   â”œâ”€â”€ dataset2/            â†’ Phase 2: Dataset-based evaluation (FED, TC, etc.)
â”‚   â”‚   â”œâ”€â”€ DSTC9/
â”‚   â”‚   â”œâ”€â”€ FED/
â”‚   â”‚   â”œâ”€â”€ PC/
â”‚   â”‚   â””â”€â”€ TC/
â”œâ”€â”€ README.md               â†’ Project documentation (this file)
```

---

## ğŸ“„ Documentation

- ğŸ“˜ `Relazione LLM-Eval.pdf` â€“ Full project report
- ğŸ“° `Articolo.pdf` â€“ Original paper on LLM-Eval
- ğŸ“Š `Presentazione LLM.pptx` â€“ Slide deck
- ğŸ“ `Traccia.pdf` â€“ Project guidelines

All located inside `docs/`.

---

## ğŸ‘¥ Contributors

- Giovanni Arcangeli
- [Vittorio Ciancio](https://github.com/VittorioCiancio)
- [Marco Di Maio](https://github.com/Marco210210)

Project presented for the Artificial Intelligence course â€“ University of Salerno (2025)

---

## âœ¨ Notes

This project demonstrates the limitations and potential of automatic dialogue evaluation. It highlights the differences between LLM generations and emphasizes the influence of dataset structure on evaluation performance.

For questions, feel free to contact the authors or open an issue on GitHub.