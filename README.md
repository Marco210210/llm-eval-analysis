# LLM-Eval: Automatic Multi-Dimensional Evaluation of Open-Domain Dialogues

<p align="center">
  <img src="https://raw.githubusercontent.com/VittorioCiancio/LLM-Eval/main/docs/banner.png" width="600"/>
</p>

## Overview
This project investigates and applies **LLM-EVAL**, a unified and multi-dimensional evaluation method for open-domain dialogues generated by Large Language Models (LLMs). We assess how well different LLMs can predict human-like quality scores over dialogues and explore how various datasets impact model performance.

> **Presented by:** Giovanni Arcangeli, Vittorio Ciancio, Marco Di Maio  
> **Course:** Artificial Intelligence (Second Part) â€“ January 2025  
> **Supervised by:** Prof. V. Deufemia, Dr. G. Cimino

---

## ðŸ” Objectives
- Understand and apply **LLM-EVAL** evaluation methodology.
- Evaluate four LLMs (Claude 3, Claude 3.5, GPT-4o, GPT-4o-mini) on the `ConvAI2` dataset.
- Compare performance across datasets: `FED`, `PC`, `TC`, `DSTC9` using Claude 3.
- Use consistent prompting and metrics across all experiments.

## ðŸ§  Models Evaluated
- **Claude 3** *(Anthropic, Haiku variant)*
- **Claude 3.5** *(Anthropic, Haiku variant)*
- **GPT-4o** *(OpenAI)*
- **GPT-4o-mini** *(lighter variant for low-resource setups)*

## ðŸ“Š Evaluation Metrics
- **Accuracy**
- **Cohenâ€™s Kappa**
- **Spearman Correlation**
- **Pearson Correlation**
- **Kendall Tau Correlation**

## ðŸ“ Project Structure
```
.
â”œâ”€â”€ docs/                  # Project documentation and slides
â”œâ”€â”€ prog/
â”‚   â”œâ”€â”€ dataset1/          # Model-wise evaluation on ConvAI2
â”‚   â”‚   â”œâ”€â”€ Claude3/
â”‚   â”‚   â”œâ”€â”€ Claude3-5/
â”‚   â”‚   â”œâ”€â”€ GPT-4o/
â”‚   â”‚   â””â”€â”€ GPT-4o-mini/
â”‚   â”œâ”€â”€ dataset2/          # Dataset-wise evaluation with Claude 3
â”‚   â”‚   â”œâ”€â”€ DSTC9/
â”‚   â”‚   â”œâ”€â”€ FED/
â”‚   â”‚   â”œâ”€â”€ PT/
â”‚   â”‚   â””â”€â”€ TC/
â”‚   â”œâ”€â”€ convai2_data.json  # Main dialogue dataset
â”‚   â””â”€â”€ requirements.txt   # Python dependencies
```

## ðŸ§ª Setup Instructions
```bash
# 1. Clone repository
https://github.com/yourusername/LLM-Eval.git
cd LLM-Eval/prog

# 2. Create virtual environment
python -m venv llm_eval_env
source llm_eval_env/bin/activate  # (or .\llm_eval_env\Scripts\activate on Windows)

# 3. Install dependencies
pip install -r requirements.txt

# 4. Add your API key in a `.env` file
ANTHROPIC_API_KEY=your_key_here
```

> Note: The `.env`, `llm_eval_env/` folder and `key_got.txt` are gitignored.

## ðŸš€ How to Run
Each folder contains:
- `valutazione_*.py`: core evaluation script
- `confronto_*.py`: comparison logic
- `risultati_*.json`: results file
- `confronto_risultati_*.json`: formatted outputs

Simply activate the virtual environment and run:
```bash
python valutazione_claude3.py
python confronto_claude_3.py
```

## ðŸ“ˆ Key Findings
- **Claude 3.5**: Best accuracy (25.99%) on ConvAI2 dataset.
- **GPT-4o-mini**: Highest correlation with human ratings.
- **FED (Dialogue-level)**: Most consistent dataset for evaluating coherence and fluency.
- **DSTC9**: Difficult to evaluate due to lack of turn-level annotations.

## ðŸ“˜ Documentation
You can find all supplementary materials inside the `docs/` folder:
- ðŸ“„ `Relazione LLM-Eval.pdf`
- ðŸ“° `Articolo.pdf` *(LLM-Eval original paper)*
- ðŸ“Š `Presentazione LLM.pptx`
- ðŸ“ `Traccia.pdf`

## ðŸ§  Reference Prompt (used across all models)
```text
Score the following dialogue generated on a continuous scale from 1 to 5.
Dialogue: {dialogue}
```

## ðŸ“œ License
This project is for educational and academic purposes only. All datasets used are publicly available.

---

> ðŸ“« For questions or contributions, feel free to contact one of the authors or open a GitHub issue.

<p align="center"><i>LLM-Eval â€” Valutazione automatica dei dialoghi con modelli linguistici di grandi dimensioni</i></p>

